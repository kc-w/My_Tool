MapReduce是一种编程模型，用于大规模数据集（大于1TB）的并行运算。
概念"Map（映射）"和"Reduce（归约）"，是它的主要思想，都是从函数式编程语言里借来的，还有从矢量编程语言里借来的特性。
它方便了编程人员在不会分布式并行编程的情况下，将自己的程序运行在分布式系统上。 

定义:
	MapReduce是面向大数据并行处理的计算模型、框架和平台，它隐含了以下三层含义：
		1:MapReduce是一个基于集群的高性能并行计算平台。它允许用市场上普通的商用服务器构成一个包含数千个节点的分布和并行计算集群。
		2:MapReduce是一个并行计算与运行软件框架。能自动完成计算任务的并行化处理，自动划分计算数据和计算任务，
			在集群节点上自动分配和执行任务以及收集计算结果，将数据分布存储、数据通信、容错处理等并行计算涉及到的很多系统底层的复杂细节交由系统负责处理，大大减少了软件开发人员的负担。
		3:MapReduce是一个并行程序设计模型与方法。它借助于函数式程序设计语言Lisp的设计思想，提供了一种简便的并行程序设计方法，
			用Map和Reduce两个函数编程实现基本的并行计算任务，提供了抽象的操作和并行编程接口，以简单方便地完成大规模数据的编程和计算处理。

地位:
	MapReduce的推出给大数据并行处理带来了巨大的革命性影响，使其已经成为事实上的大数据处理的工业标准。
	尽管MapReduce还有很多局限性，但人们普遍公认，MapReduce是到目前为止最为成功、最广为接受和最易于使用的大数据并行处理技术。
	它代表了第一个有别于冯·诺依曼结构的计算模型，是在集群规模而非单个机器上组织大规模计算的新的抽象模型上的第一个重大突破，是到目前为止所见到的最为成功的基于大规模计算资源的计算模型。


优点
• 编程模型简单,Map阶段和Reduce阶段,高伸缩性
• 支持横向扩展和并行处理
• 能够处理结构化和非结构化数据
• 高吞吐离线处理数据

缺点
• 不支持流式数据，
• 不支持实时计算
• 不支持复杂计算(如SVM支持向量机)
• 不支持迭代计算(如斐波拉契数列)
	
映射和化简:
	映射函数就是对一些独立元素组成的概念上的列表（例如，一个测试成绩的列表）的每一个元素进行指定的操作（有人发现所有学生的成绩都被高估了一分，它可以定义一个“减一”的映射函数，用来修正这个错误）。
	事实上，每个元素都是被独立操作的，而原始列表没有被更改，因为这里创建了一个新的列表来保存新的答案。这就是说，Map操作是可以高度并行的。
	
	化简操作指的是对一个列表的元素进行适当的合并（如果有人想知道班级的平均分该怎么做？它可以定义一个化简函数，通过让列表中的元素跟自己的相邻的元素相加的方式把列表减半，如此递归运算直到列表只剩下一个元素，然后用这个元素除以人数，就得到了平均分）。

Map函数:
	接受一个键值对（key-value pair），产生一组中间键值对。MapReduce框架会将map函数产生的中间键值对里键相同的值传递给一个reduce函数。
Reduce函数:
	接受一个键，以及相关的一组值，将这组值进行合并产生一组规模更小的值。

Map数据输入
	Map阶段由一定数量的Map Task组成
		• 输入数据会被split切分成多份
		• HDFS默认block大小
		• Hadoop1.0 64MB
		• Hadoop2.0 128MB
		• 默认将文件解析为<key , value>对的实现是TextInputFormat
		• Key为偏移量
		• Value为每一行内容
	MapTask数量
		• 一个split对应一个MapTask，默认情况下一个block对应一个split
		
		
Reduce数据输入
    •Partitioner决定了哪个reduce会接收到Map输出的<key , value>对
    •Hadoop中默认的Partitioner实现为HashPartitioner
    •计算公式
	• Key的哈希值对ReduceTask数量取余
        Partitioner可以自定义
	


Shuffle Map端
    Map端会源源不断的把数据输入到一个环形内存缓冲区,达到阈值(默认80%)时,新启动一个线程,
    把内存缓冲区的数据溢写到磁盘,并在溢出的过程中,对数据进行Partitioner分组,对于每个组，按照key排序
    Map处理完成后,对溢出到磁盘上的多个文件进行Merge操作,合并为一个大文件和一个索引文件


Shuffle Reduce端
    Map端完成之后后暴露一个Http Server给Reduce端获取数据使用,Reduce启动拷贝线程从各个Map端拷贝结果
    ,(产生大量的IO开销),一边拷贝一边进行Merge操作(归并排序)


主要功能:
	数据划分和计算任务调度：
		系统自动将一个作业（Job）待处理的大数据划分为很多个数据块，每个数据块对应于一个计算任务（Task），并自动调度计算节点来处理相应的数据块。
		作业和任务调度功能主要负责分配和调度计算节点（Map节点或Reduce节点），同时负责监控这些节点的执行状态，并负责Map节点执行的同步控制。
	数据/代码互定位：
		为了减少数据通信，一个基本原则是本地化数据处理，即一个计算节点尽可能处理其本地磁盘上所分布存储的数据，这实现了代码向 数据的迁移；
		当无法进行这种本地化数据处理时，再寻找其他可用节点并将数据从网络上传送给该节点（数据向代码迁移），但将尽可能从数据所在的本地机架上寻 找可用节点以减少通信延迟。
	系统优化：
		为了减少数据通信开销，中间结果数据进入Reduce节点前会进行一定的合并处理；
		一个Reduce节点所处理的数据可能会来自多个 Map节点，为了避免Reduce计算阶段发生数据相关性，Map节点输出的中间结果需使用一定的策略进行适当的划分处理，保证相关性数据发送到同一个 Reduce节点；
		此外，系统还进行一些计算性能优化处理，如对最慢的计算任务采用多备份执行、选最快完成者作为结果。
	出错检测和恢复：
		以低端商用服务器构成的大规模MapReduce计算集群中，节点硬件（主机、磁盘、内存等）出错和软件出错是常态，因此 MapReduce需要能检测并隔离出错节点，并调度分配新的节点接管出错节点的计算任务。
		同时，系统还将维护数据存储的可靠性，用多备份冗余存储机制提高数据存储的可靠性，并能及时检测和恢复出错的数据。

主要技术特征:
	向“外”横向扩展，而非向“上”纵向扩展:
		MapReduce集群的构建完全选用价格便宜、易于扩展的低端商用服务器，而非价格昂贵、不易扩展的高端服务器。
	失效被认为是常态:
		MapReduce集群中使用大量的低端服务器，因此，节点硬件失效和软件出错是常态，因而一个良好设计、具有高容错性的并行计算系统不能因为节点失效而影响计算服务的质量，任何节点失效都不应当导致结果的不一致或不确定性；
		任何一个节点失效时，其他节点要能够无缝接管失效节点的计算任务；当失效节 点恢复后应能自动无缝加入集群，而不需要管理员人工进行系统配置。
	把处理向数据迁移:
		传统高性能计算系统通常有很多处理器节点与一些外存储器节点相连，如用存储区域网络连接的磁盘阵列，因此，大规模数据处理时外存文件数据I/O访问会成为一个制约系统性能的瓶颈。
		为了减少大规模数据并行计算系统中的数据 通信开销，把数据传送到处理节点（数据向处理器或代码迁移），应当考虑将处理向数据靠拢和迁移。
		MapReduce采用了数据/代码互定位的技术方法，计算节点将首先尽量负责计算其本地存储的数据，以发挥数据本地化特点，仅当节点无法处理本地数据时，再采用就近原则寻找其他可用计算节点，并把数据传送到该可用计算节点。
	顺序处理数据、避免随机访问数据:
		大规模数据处理的特点决定了大量的数据记录难以全部存放在内存，而通常只能放在外存中进行处理。由于磁盘的顺序访问要远比随机访问快得多，因此 MapReduce主要设计为面向顺序式大规模数据的磁盘访问处理。
		为了实现面向大数据集批处理的高吞吐量的并行处理，MapReduce可以利用集群中的大量数据存储节点同时访问数据，以此利用分布集群中大量节点上的磁盘集合提供高带宽的数据访问和传输。
	为应用开发者隐藏系统层细节:
		MapReduce提供了一种抽象机制将程序员与系统层细节隔离开来，程序员仅需描述需要计算什么，而具体怎么去计算就交由系统的执行框架处理，这样程序员可从系统层细节中解放出来，而致力于其应用本身计算问题的算法设计。
	平滑无缝的可扩展性
		理想的算法应当能随着数据规模的扩大而表现出持续的有效性，性能上的下降程度应与数据规模扩大的倍数相当；
		在集群规模上，要求算法的计算性能应能随着节点数的增加保持接近线性程度的增长。绝大多数现有的单机算法都达不到 以上理想的要求；
		对于很多计算问题，基于MapReduce的计算性能可随节点数目增长保持近似于线性的增长。


MapReduce数据本地性问题
    HDFS上同一份文件会有多份拷贝(默认3份)
    MapReduce调度原则
        • 在就近的节点上启动Map Task任务
    数据本地性的级别
        • Node Local:Map Task和数据在同一个节点上
        • Rack Local:Map Task和数据在同一个机架上
        • Different Rack:Map Task和数据既不在同一个节点又不在同一个机架

MapReduce容错性
    Task运行失败
        1:Map Task失败
            • MRAppMaster重启Map Task，Map Task没有依赖性
        2:Reduce Task失败
            • MRAppMaster重启Reduce Task，Map Task的输出保存在磁盘上
    Task所在的节点挂了
        • 在另外的一个节点上重启所有在挂掉的节点上曾经运行过的任务
    Task运行缓慢
        • 通常是由于硬件损坏、软件BUG或者配置错误导致
        • 单个Task运行缓慢会显著影响整体作业运行时间
        解决方案：推测执行
            • 在另外一个节点上启动相同的任务，谁先完成就kill掉另外一个节点上的任务
            • 无法启动推测执行的情况：写入数据库


缺陷:
	MRv1是MapReduce的第一个版本,当集群包含的节点超过 4,000 个时（其中每个节点可能是多核的），就会表现出一定的不可预测性。
	其中一个问题是级联故障，由于要尝试复制数据和重载活动的节点，所以一个故障会通过网络泛洪形式导致整个集群严重恶化。
	MRv1的最大问题是多租户。随着集群规模的增加，一种可取的方式是为这些集群采用各种不同的模型。
	MRv1的节点专用于Hadoop，所以可以改变它们的用途以用于其他应用程序和工作负载。
	当大数据和 Hadoop 成为云部署中一个更重要的使用模型时，这种能力也会增强，因为它允许在服务器上对 Hadoop 进行物理化，而无需虚拟化且不会增加管理、计算和输入/输出开销。


MapReduce V1的Master/Slave结构
    JobTracker
        • 一个集群一个，安装在Master节点上
        • 管理MapReduce作业(Job)
        • 向TaskTracker分发任务(Task)
    TaskTracker
        • 每个Slave节点一个
        • 运行监控Map/Reduce任务


MapReduce V1存在的问题
    Job类型
        • 只能是MapReduce任务
    集群资源管理
        • 只有一个JobTracker，SPOF
        • 每个Slave机器可运行的最大Map Task和Reduce Task数量固定
    难以共享集群资源
        • Spark Storm Impala




Map Task和Reduce Task数目调整
    Map Task数目
        • Map读取文件时，通过InputFormat计算分割文件
        • Split大小由以下三个参数决定
        • dfs.blocksize HDFS Block大小 128M
        • mapreduce.input.fileinputformat.split.minsize 1 划分最小字节数
        • Mapreduce.input.fileinputformat.split.maxsize 最大 划分最大字节数
        • 计算公式:return Math.max(minSize,Math.min(maxSize,blockSize))
    Reduce Task数目
        • 默认每个作业Reduce Task数目可以通过mapreduce.job.reduce控制
        • 在每个作业中也可以通过job.setNumReduceTask(Int number)进行控制


容错参数调整
    Mapreduce.map.maxattempts   map最大的尝试次数,默认为4
    Mapreduce.reduce.maxattempts   reduce最大的尝试次数,默认为4
    Mapreduce.am.max-attempts   MRApplication最大的尝试次数,默认为2


内存参数调整
    mapreduce.map.memory.mb 控制分配给Map内存的上限,如果超出内存将会报错,并将进程结束
    mapreduce.map.java.opts 控制Map堆内存大小
    mapreduce.reduce.memory.mb  控制分配给Reduce内存的上限,如果超出内存将会报错,并将进程结束
    mapreduce.reduce.java.opts 控制Reduce堆内存大小
    yarn.app.mapreduce.am.resource.mb   控制分配给MRApplicationMaster内存上限
    yarn.app.mapreduce.am.command-opts   控制分配给MRApplicationMaster堆内存上限
    mapreduce.job.heap.memory-mb.ratio  Heap大小占Container大小的比例,默认为0.8
    yarn.nodemanager.vmem-pmem-artio   物理内存和虚拟内存的比率,默认为2:1