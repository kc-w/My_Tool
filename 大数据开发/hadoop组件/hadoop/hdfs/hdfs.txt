Hadoop分布式文件系统(HDFS)被设计成适合运行在通用硬件上的分布式文件系统。

HDFS有着高容错性的特点，并且设计用来部署在低廉的硬件上。
HDFS提供高吞吐量来访问应用程序的数据，适合那些有着超大数据集的应用程序。
HDFS放宽了POSIX的要求这样可以实现流的形式访问文件系统中的数据。

硬件故障
	硬件故障是常态，而不是异常。
	整个HDFS系统将由数百或数千个存储着文件数据片段的服务器组成。
	实际上它里面有非常巨大的组成部分，每一个组成部分都很可能出现故障，这就意味着HDFS里的总是有一些部件是失效的，
	因此，故障的检测和自动快速恢复是HDFS一个很核心的设计目标。

数据访问
	运行在HDFS之上的应用程序必须流式地访问它们的数据集，它不是运行在普通文件系统之上的普通程序。
	HDFS被设计成适合批量处理的，而不是用户交互式的。
	重点是在数据吞吐量，而不是数据访问的反应时间，POSIX的很多硬性需求对于HDFS应用都是非必须的，
	去掉POSIX一小部分关键语义可以获得更好的数据吞吐率。
大数据集
	运行在HDFS之上的程序有很大量的数据集。典型的HDFS文件大小是GB到TB的级别。所以，HDFS被调整成支持大文件。
	它应该提供很高的聚合数据带宽，一个集群中支持数百个节点，一个集群中还应该支持千万级别的文件。

大量小文件的危害
	block信息都存储在namenode的元数据中,太多小文件会占用namenode太多空间
	解决方式:
		1,减少小文件
		2,archive打包多个小文件为一个文件
		3,使用hbase存储
大数据量下的namenode有什么问题
	启动时间长
	性能下降
	namenode jvm FGC风险高
		解决方案
			1,预估namenode内存需求,
			2,合并小文件
			3,调整合适的block大小
		

	
namenode节点和datanode节点
	HDFS是一个主从结构，一个HDFS集群是有一个namenode节点，它是一个管理文件命名空间和调节客户端访问文件的主服务器，。
	HDFS内部机制是将一个文件分割成一个或多个块，这些块被存储在一组datanode节点中。
	namenode节点用来操作文件命名或目录操作，如打开，关闭，重命名等等。它同时确定块与datanode的映射。
	datanode节点负责来自客户端的读写请求。datanode同时还要执行块的创建，删除，和来自namenode节点的块复制指令。
	namenode和datanode都是运行在机器上的进程。
	
数据复制
	HDFS设计成能可靠地在集群中大量机器之间存储大量的文件，它以块序列的形式存储文件。
	文件中除了最后一个块，其他块都有相同的大小。属于文件的块为了故障容错而被复制。
	块的大小和复制数是以文件为单位进行配置的，应用可以在文件创建时或者之后修改配置参数。
	HDFS中的文件是一次写的，并且任何时候都只有一个写操作。
	namenode节点负责处理所有的块复制相关的决策。它周期性地接受集群中datenode节点的心跳和块报告。
	一个心跳的到达表示这个datanode节点是正常的。一个块报告包括该datanode节点上所有块的列表。
	
数据复制策略
	在开始的时候，每一个datanode自检它所属的机架id，然后在向namenode注册的时候告知它的机架id。
	文件的流水式复制:
		当客户端写数据到HDFS文件中时， 
		数据首先被写入本地文件中，假设HDFS文件的复制数是3，当本地文件堆积到一块大小的数据，客户端从namenode获得一个datanode的列表。
		这个列表也包含存放数据块副本的datanode。当客户端刷新数据块到第一个datanode。
		第一个datanode开始以4kb为单元接收数据，将每一小块都写到本地库中，同时将每一小块都传送到列表中的第二个datanode。
		同理，第二个datanode将小块数据写入本地库中同时传给第三个datanode，第三个datanode直接写到本地库中。
		一个datanode在接前一个节点数据的同时，还可以将数据流水式传递给下一个节点，所以，数据是流水式地从一个datanode传递到下一个。
	第一个方式(简单但不是最优):
		将副本放置在不同的机架上，这就防止了机架故障时数据的丢失，并且在读数据的时候可以充分利用不同机架的带宽。
		这个方式均匀地将复制分散在集群中，这就简单地实现了组建故障时的负载均衡。
		然而这种方式增加了写的成本，因为写的时候需要跨越多个机架传输文件块。
	第二个方式:
		默认的HDFS block(128MB)放置策略在最小化写开销和最大化数据可靠性、可用性以及总体读取带宽之间进行了一些折中。
		HDFS的副本放置策略是将第一个副本放在本地节点，将第二个副本放到本地机架上的另外一个节点而将第三个副本放到不同机架上的节点。
		这种方式减少了机架间的写流量，从而提高了写的性能。机架故障的几率远小于节点故障。
		这种方式不影响数据可靠性和可用性的限制，并且它确实减少了读操作的网络聚合带宽，因为文件块仅存在两个不同的机架， 而不是三个。
		文件的副本不是均匀地分布在机架当中，1/3在同一个节点上，1/3副本在同一个机架上，另外1/3均匀地分布在其他机架上。
		这种方式提高了写的性能，并且不影响数据的可靠性和读性能。

文件副本的选择:
	为了尽量减小全局的带宽消耗读延迟，HDFS尝试返回给一个读操作离它最近的副本。
	假如在读节点的同一个机架上就有这个副本，就直接读这个，如果HDFS集群是跨越多个数据中心，那么本地数据中心的副本优先于远程的副本。

hdfss安全模式
	在启动的时候，namenode进入一个叫做安全模式的特殊状态。安全模式中不允许发生对文件块的操作,但可以查看已有的文件块。
	
文件元数据的持久化
	对文件操作的持久化:
		HDFS的命名空间是由namenode来存储的。
		namenode使用叫做EditLog的事务日志来记录文件系统元数据的改变，如在HDFS中创建一个文件，namenode将会在EditLog中插入一条记录来记录这个改变。
		类似地，改变文件的复制因子也会向EditLog中插入一条记录。namenode在本地文件系统中用一个文件来存储这个EditLog。
		整个文件系统命名空间，包括文件块的映射表和文件系统的配置都存在一个叫FsImage的文件中，FsImage也存放在namenode的本地文件系统中。
	
	namenode重启:
		namenode在内存中保留一个完整的文件系统命名空间和文件块的映射表的镜像。这个元数据被设计成紧凑的。
		namenode启动时，它将从磁盘中读取FsImage和EditLog，将EditLog中的事务操作记录到FsImage的仿内存空间，
		然后将新的FsImage刷新到本地磁盘中，因为事务已经被处理并已经持久化的FsImage中，然后就可以删除旧的EditLog。
		这个过程叫做检查点。当前实现中，检查点仅在namenode启动的时候发生，支持周期性的检查点。


存储空间回收:
	文件删除和恢复删除:
		当一个文件被用户或程序删除时，它并没有立即从HDFS中删除(默认的策略是删除在此目录存放超过6小时的文件)。
		HDFS将它重新命名后转存到/trash目录下，这个文件只要还在/trash目录下保留就可以重新快速恢复。
		文件在/trash中存放的时间是可配置的。存储时间超时后，namenode就将目标文件从名字空间中删除，同时此文件关联的所有文件块都将被释放。

		
异常处理
	HDFS的主要目标就是在存在故障的情况下也能可靠地存储数据。三个最常见的故障是namenode故障，datanode故障和网络断开。
	重新复制:
		一个datanode周期性发送一个心跳包到namenode。网络断开会造成一组datanode子集和namenode失去联系。
		namenode根据缺失的心跳信息判断故障情况,将这些datanode标记为死亡状态，不再将新的IO请求转发到这些datanode上，
		这些datanode上的数据将对HDFS不再可用，可能会导致一些块的复制数降低到指定的值。
		namenode检查所有的需要复制的块，并开始复制他们到其他的datanode上。重新复制在有些情况下是不可或缺的，
		例如：datanode失效，副本损坏，datanode磁盘损坏或者文件的复制数增大。
	数据正确性:
		从datanode上取一个文件块有可能是坏块，坏块的出现可能是存储设备错误，网络错误或者软件的漏洞。
		当一个客户端创建一个HDFS文件时，它会为每一个文件块计算一个校验码并将校验码存储在同一个HDFS命名空间下一个单独的隐藏文件中。
		当客户端访问这个文件时，它根据对应的校验文件来验证从datanode接收到的数据。
		如果校验失败，客户端可以选择从其他拥有该块副本的datanode获取这个块。
	元数据失效:
		FsImage和Editlog是HDFS的核心数据结构。这些文件的损坏会导致整个集群的失效。
		因此，namenode可以配置成支持多个FsImage和EditLog的副本。任何FsImage和EditLog的更新都会同步到每一份副本中。
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
